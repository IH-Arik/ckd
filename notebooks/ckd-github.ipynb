{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python3\n\"\"\"\nChronic Kidney Disease (CKD) Prediction Pipeline\n===============================================\n\nA comprehensive machine learning pipeline for CKD detection and prognosis\nusing hybrid clinical decision support.\n\nAuthor: CKD Research Team\nDate: 2024\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# =============================================================================\n# STEP 1: IMPORTS & SETUP\n# =============================================================================\n\nprint(\"üè• CKD Prediction Pipeline - Starting...\")\nprint(\"=\" * 60)\n\n# Basic libraries\nimport pandas as pd\nimport numpy as np\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.utils import resample\n\n# Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (\n    RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\n\n# Boosting libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\n# Survival analysis\ntry:\n    from sksurv.linear_model import CoxPHSurvivalAnalysis\n    from sksurv.ensemble import RandomSurvivalForest\n    SURVIVAL_AVAILABLE = True\n    print(\"‚úÖ Survival analysis libraries loaded\")\nexcept ImportError:\n    print(\"‚ö†Ô∏è  Survival analysis libraries not available - skipping prognosis models\")\n    SURVIVAL_AVAILABLE = False\n\n# Calibration\nfrom sklearn.calibration import CalibratedClassifierCV\n\n# Metrics\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, average_precision_score, brier_score_loss,\n    precision_recall_curve, mean_absolute_error\n)\nfrom sklearn.calibration import calibration_curve\n\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.backends.backend_pdf import PdfPages\n\n# Explainability\nimport shap\nimport lime.lime_tabular\n\n# Misc\nimport joblib\n\nprint(\"‚úÖ All libraries imported successfully\")\n\n# =============================================================================\n# STEP 2: LOAD DATASETS\n# =============================================================================\n\nprint(\"\\nüìä Loading Datasets...\")\n\ntry:\n    # Load datasets\n    kidney_df = pd.read_csv(\"/kaggle/input/ckdisease/kidney_disease.csv\")\n    clinical_df = pd.read_excel(\"/kaggle/input/prone4911/pone.0199920.s002.xlsx\")\n    \n    print(f\"‚úÖ Kidney dataset shape: {kidney_df.shape}\")\n    print(f\"‚úÖ Clinical dataset shape: {clinical_df.shape}\")\n    \nexcept Exception as e:\n    print(f\"‚ùå Dataset loading error: {e}\")\n    print(\"Please ensure the Kaggle datasets are properly mounted\")\n    exit(1)\n\n# =============================================================================\n# STEP 3: KIDNEY DATASET PREPROCESSING (DETECTION)\n# =============================================================================\n\nprint(\"\\nüîß Preprocessing Kidney Dataset (Detection)...\")\n\n# Clean target\nkidney_df[\"classification\"] = kidney_df[\"classification\"].str.strip().map({\"ckd\": 1, \"notckd\": 0})\n\n# Replace \"?\" with NaN\nkidney_df = kidney_df.replace(\"?\", np.nan)\n\n# Convert numeric-like columns\nnum_cols = ['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','pcv','wc','rc']\nfor col in num_cols:\n    kidney_df[col] = pd.to_numeric(kidney_df[col], errors=\"coerce\")\n\n# Clinically aware imputation\nkidney_df[\"sod\"] = kidney_df[\"sod\"].fillna(140)  # sodium normal mean\nkidney_df[\"pot\"] = kidney_df[\"pot\"].fillna(4.5)  # potassium normal mean\nkidney_df[\"hemo\"] = kidney_df[\"hemo\"].fillna(13.5)  # hemoglobin normal mean\n\nfor col in [\"pcv\",\"wc\",\"rc\"]:\n    kidney_df[col] = kidney_df[col].fillna(kidney_df[col].median())\n\n# Feature engineering\nkidney_df[\"urea_creatinine_ratio\"] = kidney_df[\"bu\"] / (kidney_df[\"sc\"]+1e-5)\nkidney_df[\"anemia_flag\"] = (kidney_df[\"hemo\"] < 12).astype(int)\nkidney_df[\"htn_dm_interaction\"] = ((kidney_df[\"htn\"]==\"yes\") & (kidney_df[\"dm\"]==\"yes\")).astype(int)\n\n# Outlier handling (winsorize)\nkidney_df[\"sc\"] = np.clip(kidney_df[\"sc\"], 0, 15)  # serum creatinine\nkidney_df[\"bu\"] = np.clip(kidney_df[\"bu\"], 0, 300) # blood urea\nkidney_df[\"bp\"] = np.clip(kidney_df[\"bp\"], 40, 200)\n\n# Split features / target\nX_kd = kidney_df.drop(columns=[\"id\",\"classification\"])\ny_kd = kidney_df[\"classification\"]\n\nprint(f\"‚úÖ Kidney dataset ready. Features: {X_kd.shape[1]}\")\nprint(f\"‚úÖ Target distribution: {y_kd.value_counts().to_dict()}\")\n\n# =============================================================================\n# STEP 4: CLINICAL DATASET PREPROCESSING (PROGNOSIS)\n# =============================================================================\n\nprint(\"\\nüîß Preprocessing Clinical Dataset (Prognosis)...\")\n\n# Clean column names\nclinical_df = clinical_df.rename(columns={\"HistoryHTN \": \"HistoryHTN\"})\n\n# Structured survival outcome (event, time)\ny_clinical = np.array([\n    (bool(event), time) for event, time in zip(clinical_df[\"EventCKD35\"], clinical_df[\"TimeToEventMonths\"])\n], dtype=[(\"event\", \"bool\"), (\"time\", \"int\")])\n\n# Feature Engineering\nclinical_df[\"BMI_Category\"] = pd.cut(\n    clinical_df[\"BMIBaseline\"],\n    bins=[0, 25, 30, 100],\n    labels=[\"Normal\", \"Overweight\", \"Obese\"]\n)\n\nclinical_df[\"Metabolic_Syndrome\"] = (\n    clinical_df[\"HistoryDiabetes\"] +\n    clinical_df[\"HistoryHTN\"] +\n    clinical_df[\"HistoryObesity\"] +\n    clinical_df[\"HistoryDLD\"]\n)\n\n# Age group dummies (align with Age.3.categories)\nclinical_df = pd.get_dummies(clinical_df, columns=[\"Age.3.categories\"], drop_first=True)\n\n# Missing Value Imputation\nclinical_df[\"HgbA1C\"] = clinical_df[\"HgbA1C\"].fillna(clinical_df[\"HgbA1C\"].median())\nclinical_df[\"TriglyceridesBaseline\"] = clinical_df[\"TriglyceridesBaseline\"].fillna(\n    clinical_df[\"TriglyceridesBaseline\"].median()\n)\n\n# Outlier Handling (Winsorize)\nclinical_df[\"CreatnineBaseline\"] = np.clip(clinical_df[\"CreatnineBaseline\"], 20, 800)\nclinical_df[\"CholesterolBaseline\"] = np.clip(clinical_df[\"CholesterolBaseline\"], 2, 15)\nclinical_df[\"TriglyceridesBaseline\"] = np.clip(clinical_df[\"TriglyceridesBaseline\"], 0.2, 10)\n\n# Feature / Target Split\nX_clinical = clinical_df.drop(columns=[\"StudyID\", \"EventCKD35\", \"TimeToEventMonths\"])\n\nprint(f\"‚úÖ Clinical dataset ready. Features: {X_clinical.shape[1]}\")\nprint(f\"‚úÖ Survival outcome shape: {y_clinical.shape}\")\nprint(f\"‚úÖ Event rate: {y_clinical['event'].mean():.3f}\")\n\n# =============================================================================\n# STEP 5: DETECTION MODELS - KIDNEY DATASET\n# =============================================================================\n\nprint(\"\\nü§ñ Training Detection Models (Kidney Dataset)...\")\n\n# Train-Test Split\nX_train_kd, X_test_kd, y_train_kd, y_test_kd = train_test_split(\n    X_kd, y_kd, test_size=0.2, stratify=y_kd, random_state=42\n)\n\n# Preprocessing Pipeline for Kidney Dataset\nnumeric_features_kd = X_kd.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_features_kd = X_kd.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# Create preprocessing pipeline with imputation\npreprocessor_kd = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline([\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())\n        ]), numeric_features_kd),\n        ('cat', Pipeline([\n            ('imputer', SimpleImputer(strategy='most_frequent')),\n            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n        ]), categorical_features_kd)\n    ]\n)\n\n# Preprocess\nX_train_proc = preprocessor_kd.fit_transform(X_train_kd)\nX_test_proc = preprocessor_kd.transform(X_test_kd)\n\n# Check for any remaining NaN values\nprint(f\"NaN values in X_train_proc: {np.isnan(X_train_proc).sum()}\")\nprint(f\"NaN values in X_test_proc: {np.isnan(X_test_proc).sum()}\")\n\n# Apply SMOTE\nsmote = BorderlineSMOTE(random_state=42)\nX_train_res, y_train_res = smote.fit_resample(X_train_proc, y_train_kd)\n\nprint(f\"‚úÖ SMOTE applied successfully\")\nprint(f\"‚úÖ Resampled training set shape: {X_train_res.shape}\")\nprint(f\"‚úÖ Resampled target distribution: {np.bincount(y_train_res)}\")\n\n# Define Base Classifiers\nbase_classifiers = {\n    \"Logistic Regression\": LogisticRegression(random_state=42),\n    \"Random Forest\": RandomForestClassifier(random_state=42),\n    \"XGBoost\": xgb.XGBClassifier(random_state=42),\n    \"LightGBM\": lgb.LGBMClassifier(random_state=42),\n    \"CatBoost\": cb.CatBoostClassifier(random_state=42, verbose=False),\n    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n    \"Extra Trees\": ExtraTreesClassifier(random_state=42),\n    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n    \"Naive Bayes\": GaussianNB(),\n    \"KNN\": KNeighborsClassifier(),\n    \"MLP\": MLPClassifier(random_state=42),\n    \"Balanced RF\": BalancedRandomForestClassifier(random_state=42),\n    \"Easy Ensemble\": EasyEnsembleClassifier(random_state=42)\n}\n\n# Cross-Validation Evaluation\ncv_results = {}\ncv_scores = {}\n\nprint(\"üîÑ Running 5-Fold Stratified Cross-Validation...\")\ncv_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor name, clf in base_classifiers.items():\n    # Create pipeline: preprocessor -> SMOTE -> classifier\n    cv_pipeline = ImbPipeline([\n        ('preprocessor', preprocessor_kd),\n        ('smote', BorderlineSMOTE(random_state=42)),\n        ('classifier', clf)\n    ])\n    \n    # Cross-validation scores\n    cv_auc = cross_val_score(cv_pipeline, X_kd, y_kd, cv=cv_folds, scoring='roc_auc')\n    cv_f1 = cross_val_score(cv_pipeline, X_kd, y_kd, cv=cv_folds, scoring='f1')\n    cv_acc = cross_val_score(cv_pipeline, X_kd, y_kd, cv=cv_folds, scoring='accuracy')\n    \n    cv_results[name] = {\n        'AUC_mean': cv_auc.mean(),\n        'AUC_std': cv_auc.std(),\n        'F1_mean': cv_f1.mean(),\n        'F1_std': cv_f1.std(),\n        'Accuracy_mean': cv_acc.mean(),\n        'Accuracy_std': cv_acc.std()\n    }\n    \n    cv_scores[name] = cv_auc.mean()\n\nprint(\"‚úÖ Cross-validation completed!\")\n\n# Train individual classifiers\nresults_detection = []\nfitted_classifiers = {}\n\nfor name, clf in base_classifiers.items():\n    clf.fit(X_train_res, y_train_res)\n    fitted_classifiers[name] = clf\n    \n    y_pred = clf.predict(X_test_proc)\n    y_prob = clf.predict_proba(X_test_proc)[:,1]\n    \n    acc = accuracy_score(y_test_kd, y_pred)\n    f1 = f1_score(y_test_kd, y_pred)\n    prec = precision_score(y_test_kd, y_pred)\n    rec = recall_score(y_test_kd, y_pred)\n    auc_score = roc_auc_score(y_test_kd, y_prob)\n    pr_auc = average_precision_score(y_test_kd, y_prob)\n    brier = brier_score_loss(y_test_kd, y_prob)\n    \n    # Add CV scores\n    cv_auc_mean = cv_results[name]['AUC_mean']\n    cv_auc_std = cv_results[name]['AUC_std']\n    \n    results_detection.append([name, acc, f1, prec, rec, auc_score, pr_auc, brier, cv_auc_mean, cv_auc_std])\n\nresults_detection_df = pd.DataFrame(\n    results_detection,\n    columns=[\"Model\",\"Accuracy\",\"F1\",\"Precision\",\"Recall\",\"ROC AUC\",\"PR AUC\",\"Brier Score\",\"CV AUC Mean\",\"CV AUC Std\"]\n).sort_values(by=\"CV AUC Mean\", ascending=False)\n\nprint(\"\\n=== Kidney Dataset Detection Results (with Cross-Validation) ===\")\nprint(results_detection_df)\n\n# Select Best Model and Calibrate\nbest_model_name = results_detection_df.iloc[0][\"Model\"]\nbest_model = fitted_classifiers[best_model_name]\n\n# Calibrate the best model\ncalibrated_clf = CalibratedClassifierCV(best_model, method='isotonic', cv=3)\ncalibrated_clf.fit(X_train_res, y_train_res)\n\nprint(f\"\\n‚úÖ Best Detection Model: {best_model_name}\")\nprint(f\"‚úÖ CV AUC Score: {cv_scores[best_model_name]:.3f} ¬± {cv_results[best_model_name]['AUC_std']:.3f}\")\nprint(\"‚úÖ Model calibrated for better probability estimates\")\n\n# =============================================================================\n# STEP 6: PROGNOSIS MODELS - CLINICAL DATASET\n# =============================================================================\n\n# Initialize variables for both cases\nX_test_cl = None\ny_test_cl = None\nprognosis_models = None\nprognosis_model = None\n\nif SURVIVAL_AVAILABLE:\n    print(\"\\nüîÆ Training Prognosis Models (Clinical Dataset)...\")\n    \n    # Clinical Dataset Train-Test Split\n    X_train_cl, X_test_cl, y_train_cl, y_test_cl = train_test_split(\n        X_clinical, y_clinical, test_size=0.2, random_state=42\n    )\n    \n    # Preprocessing Pipeline for Clinical Dataset\n    numeric_features_cl = X_clinical.select_dtypes(include=[np.number]).columns.tolist()\n    categorical_features_cl = X_clinical.select_dtypes(include=['object', 'category']).columns.tolist()\n    \n    # Create preprocessing pipeline with imputation\n    preprocessor_cl = ColumnTransformer(\n        transformers=[\n            ('num', Pipeline([\n                ('imputer', SimpleImputer(strategy='median')),\n                ('scaler', StandardScaler())\n            ]), numeric_features_cl),\n            ('cat', Pipeline([\n                ('imputer', SimpleImputer(strategy='most_frequent')),\n                ('encoder', OneHotEncoder(handle_unknown='ignore'))\n            ]), categorical_features_cl)\n        ]\n    )\n    \n    # Check for any remaining NaN values after preprocessing\n    X_train_cl_proc = preprocessor_cl.fit_transform(X_train_cl)\n    X_test_cl_proc = preprocessor_cl.transform(X_test_cl)\n    \n    print(f\"NaN values in X_train_cl_proc: {np.isnan(X_train_cl_proc).sum()}\")\n    print(f\"NaN values in X_test_cl_proc: {np.isnan(X_test_cl_proc).sum()}\")\n    \n    # CoxPH\n    cox_pipe = Pipeline([\n        (\"preprocessor\", preprocessor_cl),\n        (\"model\", CoxPHSurvivalAnalysis())\n    ])\n    cox_pipe.fit(X_train_cl, y_train_cl)\n    \n    # RSF\n    rsf_pipe = Pipeline([\n        (\"preprocessor\", preprocessor_cl),\n        (\"model\", RandomSurvivalForest(n_estimators=200, random_state=42))\n    ])\n    rsf_pipe.fit(X_train_cl, y_train_cl)\n    \n    # Store models\n    prognosis_models = {\n        \"Cox Proportional Hazards\": cox_pipe,\n        \"Random Survival Forest\": rsf_pipe\n    }\n    \n    # Pick RSF as best\n    prognosis_model = rsf_pipe\n    print(\"‚úÖ Prognosis model = Random Survival Forest (with OneHot categorical handling)\")\n    print(\"‚úÖ Cox Proportional Hazards trained successfully\")\n    print(\"‚úÖ Random Survival Forest trained successfully\")\n    \n    # =============================================================================\n    # PROGNOSIS MODEL EVALUATION\n    # =============================================================================\n    \n    print(\"\\nüìä Evaluating Prognosis Models...\")\n    \n    # Convert survival data to binary classification for evaluation\n    # We'll use a time horizon (e.g., 60 months) to convert survival to binary\n    horizon_months = 60\n    \n    # Create binary targets for evaluation\n    y_train_binary = (y_train_cl['time'] <= horizon_months) & (y_train_cl['event'])\n    y_test_binary = (y_test_cl['time'] <= horizon_months) & (y_test_cl['event'])\n    \n    print(f\"Binary classification target: CKD event within {horizon_months} months\")\n    print(f\"Training set event rate: {y_train_binary.mean():.3f}\")\n    print(f\"Test set event rate: {y_test_binary.mean():.3f}\")\n    \n    # Evaluate both prognosis models\n    prognosis_results = []\n    \n    for model_name, model in prognosis_models.items():\n        print(f\"\\nüîç Evaluating {model_name}...\")\n        \n        # Get survival probabilities at horizon\n        try:\n            # For CoxPH\n            if model_name == \"Cox Proportional Hazards\":\n                # CoxPH doesn't have predict_survival_function, use risk scores\n                risk_scores_train = model.predict(X_train_cl)\n                risk_scores_test = model.predict(X_test_cl)\n                \n                # Convert risk scores to probabilities (higher risk = higher probability of event)\n                # Normalize to 0-1 range\n                risk_scores_train_norm = (risk_scores_train - risk_scores_train.min()) / (risk_scores_train.max() - risk_scores_train.min())\n                risk_scores_test_norm = (risk_scores_test - risk_scores_test.min()) / (risk_scores_test.max() - risk_scores_test.min())\n                \n                y_prob_train = risk_scores_train_norm\n                y_prob_test = risk_scores_test_norm\n                \n                # Binary predictions (threshold at 0.5)\n                y_pred_train = (y_prob_train > 0.5).astype(int)\n                y_pred_test = (y_prob_test > 0.5).astype(int)\n                \n            # For Random Survival Forest\n            else:\n                # Get survival functions\n                surv_funcs_train = model.predict_survival_function(X_train_cl)\n                surv_funcs_test = model.predict_survival_function(X_test_cl)\n                \n                # Get survival probabilities at horizon\n                y_prob_train = np.array([1 - fn(horizon_months) for fn in surv_funcs_train])\n                y_prob_test = np.array([1 - fn(horizon_months) for fn in surv_funcs_test])\n                \n                # Binary predictions (threshold at 0.5)\n                y_pred_train = (y_prob_train > 0.5).astype(int)\n                y_pred_test = (y_prob_test > 0.5).astype(int)\n            \n            # Calculate metrics for training set\n            train_acc = accuracy_score(y_train_binary, y_pred_train)\n            train_f1 = f1_score(y_train_binary, y_pred_train)\n            train_recall = recall_score(y_train_binary, y_pred_train)\n            train_precision = precision_score(y_train_binary, y_pred_train)\n            train_auc = roc_auc_score(y_train_binary, y_prob_train)\n            train_mae = mean_absolute_error(y_train_binary, y_prob_train)\n            \n            # Calculate metrics for test set\n            test_acc = accuracy_score(y_test_binary, y_pred_test)\n            test_f1 = f1_score(y_test_binary, y_pred_test)\n            test_recall = recall_score(y_test_binary, y_pred_test)\n            test_precision = precision_score(y_test_binary, y_pred_test)\n            test_auc = roc_auc_score(y_test_binary, y_prob_test)\n            test_mae = mean_absolute_error(y_test_binary, y_prob_test)\n            \n            # Store results\n            prognosis_results.append({\n                'Model': model_name,\n                'Train_Accuracy': train_acc,\n                'Train_F1': train_f1,\n                'Train_Recall': train_recall,\n                'Train_Precision': train_precision,\n                'Train_AUC': train_auc,\n                'Train_MAE': train_mae,\n                'Test_Accuracy': test_acc,\n                'Test_F1': test_f1,\n                'Test_Recall': test_recall,\n                'Test_Precision': test_precision,\n                'Test_AUC': test_auc,\n                'Test_MAE': test_mae\n            })\n            \n            # Print results\n            print(f\"\\nüìà {model_name} Results:\")\n            print(\"=\" * 50)\n            print(\"TRAINING SET:\")\n            print(f\"  Accuracy:  {train_acc:.4f}\")\n            print(f\"  F1-Score:  {train_f1:.4f}\")\n            print(f\"  Recall:    {train_recall:.4f}\")\n            print(f\"  Precision: {train_precision:.4f}\")\n            print(f\"  AUC:       {train_auc:.4f}\")\n            print(f\"  MAE:       {train_mae:.4f}\")\n            print(\"\\nTEST SET:\")\n            print(f\"  Accuracy:  {test_acc:.4f}\")\n            print(f\"  F1-Score:  {test_f1:.4f}\")\n            print(f\"  Recall:    {test_recall:.4f}\")\n            print(f\"  Precision: {test_precision:.4f}\")\n            print(f\"  AUC:       {test_auc:.4f}\")\n            print(f\"  MAE:       {test_mae:.4f}\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error evaluating {model_name}: {e}\")\n            continue\n    \n    # Create results DataFrame\n    if prognosis_results:\n        prognosis_results_df = pd.DataFrame(prognosis_results)\n        prognosis_results_df = prognosis_results_df.sort_values(by='Test_AUC', ascending=False)\n        \n        print(f\"\\nüìä PROGNOSIS MODELS COMPARISON:\")\n        print(\"=\" * 80)\n        print(prognosis_results_df.round(4))\n        \n        # Save results\n        prognosis_results_df.to_csv('prognosis_results.csv', index=False)\n        print(f\"\\n‚úÖ Prognosis results saved as 'prognosis_results.csv'\")\n        \n        # Select best prognosis model based on test AUC\n        best_prognosis_model_name = prognosis_results_df.iloc[0]['Model']\n        print(f\"\\nüèÜ Best Prognosis Model: {best_prognosis_model_name}\")\n        print(f\"üèÜ Test AUC: {prognosis_results_df.iloc[0]['Test_AUC']:.4f}\")\n        \n        # Update prognosis_model to best model\n        prognosis_model = prognosis_models[best_prognosis_model_name]\n    \nelse:\n    print(\"\\n‚ö†Ô∏è  Skipping Prognosis Models (scikit-survival not available)\")\n\n# =============================================================================\n# STEP 7: HYBRID DECISION FUNCTION\n# =============================================================================\n\nprint(\"\\nüîó Creating Hybrid Decision Function...\")\n\ndef hybrid_ckd_predict(patient_kd, patient_cl, detection_model, prognosis_model, preprocessor_kd, horizon=60):\n    \"\"\"\n    Hybrid CKD Prediction Function\n    \n    Combines detection (current CKD status) and prognosis (future CKD risk) models\n    to provide comprehensive clinical decision support for CKD management.\n    \n    Parameters\n    ----------\n    patient_kd : pandas.DataFrame\n        Single patient data from kidney dataset with shape (1, n_features).\n        Contains clinical features for CKD detection (age, creatinine, etc.)\n    \n    patient_cl : pandas.DataFrame  \n        Single patient data from clinical dataset with shape (1, n_features).\n        Contains features for CKD prognosis (BMI, comorbidities, etc.)\n    \n    detection_model : sklearn.CalibratedClassifierCV\n        Trained and calibrated classification model for CKD detection.\n        Should have .predict() and .predict_proba() methods.\n    \n    prognosis_model : sklearn.pipeline.Pipeline\n        Trained survival analysis model for CKD prognosis.\n        Should have .predict_survival_function() method.\n    \n    preprocessor_kd : sklearn.compose.ColumnTransformer\n        Preprocessor for kidney dataset features.\n    \n    horizon : int, default=60\n        Prediction horizon in months for prognosis model.\n        Common values: 12, 24, 36, 60 months.\n    \n    Returns\n    -------\n    dict\n        Dictionary containing:\n        - 'Detection': str\n            \"CKD Present\" or \"CKD Not Present\"\n        - 'Detection Probability': float\n            Probability of current CKD (0-1)\n        - 'Future Risk (X months)': float\n            Risk of developing CKD in future (0-1)\n    \n    Notes\n    -----\n    Clinical Logic:\n    1. If patient already has CKD (detection=1), future risk = 100%\n    2. If patient doesn't have CKD (detection=0), use prognosis model\n    3. Risk = 1 - Survival Probability at given horizon\n    \n    Examples\n    --------\n    >>> result = hybrid_ckd_predict(\n    ...     sample_kd, sample_cl, \n    ...     calibrated_clf, prognosis_model,\n    ...     preprocessor_kd,\n    ...     horizon=60\n    ... )\n    >>> print(result['Detection'])\n    'CKD Not Present'\n    >>> print(result['Future Risk (60 months)'])\n    0.234\n    \"\"\"\n    # Preprocess patient data for detection model only\n    patient_kd_proc = preprocessor_kd.transform(patient_kd)\n    \n    # Detection\n    det_pred = detection_model.predict(patient_kd_proc)[0]\n    det_prob = detection_model.predict_proba(patient_kd_proc)[0,1]\n    \n    if det_pred == 1:  # CKD present\n        return {\n            \"Detection\": \"CKD Present\",\n            \"Detection Probability\": round(det_prob, 3),\n            \"Future Risk ({} months)\".format(horizon): 1.0  # already CKD\n        }\n    else:  # CKD not present ‚Üí run prognosis\n        if prognosis_model is not None:\n            # Prognosis model already includes preprocessor, so use raw data\n            surv_fn = prognosis_model.predict_survival_function(patient_cl)[0]\n            surv_prob = surv_fn(horizon)\n            risk_prob = 1 - surv_prob  # risk = 1 - survival\n            return {\n                \"Detection\": \"CKD Not Present\",\n                \"Detection Probability\": round(det_prob, 3),\n                \"Future Risk ({} months)\".format(horizon): round(risk_prob, 3)\n            }\n        else:\n            return {\n                \"Detection\": \"CKD Not Present\",\n                \"Detection Probability\": round(det_prob, 3),\n                \"Future Risk ({} months)\".format(horizon): \"N/A (prognosis model not available)\"\n            }\n\n# Test hybrid function\nif SURVIVAL_AVAILABLE:\n    sample_kd = X_test_kd.iloc[[0]]\n    sample_cl = X_test_cl.iloc[[0]]\n    \n    hybrid_result = hybrid_ckd_predict(\n        sample_kd, sample_cl,\n        calibrated_clf,\n        prognosis_models[\"Random Survival Forest\"],\n        preprocessor_kd,\n        horizon=60\n    )\n    \n    print(\"‚úÖ Hybrid Decision Function working\")\n    print(f\"‚úÖ Sample result: {hybrid_result}\")\nelse:\n    print(\"‚úÖ Hybrid Decision Function created (prognosis models not available)\")\n\n# =============================================================================\n# STEP 8A: EVALUATION IMPROVEMENTS (DETECTION MODEL)\n# =============================================================================\n\nprint(\"\\nüìä Evaluation Improvements (Detection Model)...\")\n\n# Calibration Curve\ny_prob_det = calibrated_clf.predict_proba(X_test_proc)[:,1]\nprob_true, prob_pred = calibration_curve(y_test_kd, y_prob_det, n_bins=10)\n\nplt.figure(figsize=(6,6))\nplt.plot(prob_pred, prob_true, marker='o', label='Detection Model')\nplt.plot([0,1],[0,1],'k--', label='Perfect Calibration')\nplt.xlabel(\"Predicted probability\")\nplt.ylabel(\"Observed frequency\")\nplt.title(\"Calibration Curve (Kidney Detection Model)\")\nplt.legend()\nplt.savefig('calibration_curve.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Precision-Recall Curve\nprec, rec, thr = precision_recall_curve(y_test_kd, y_prob_det)\nplt.figure(figsize=(6,6))\nplt.plot(rec, prec, label='PR Curve (AUC = %.3f)' % average_precision_score(y_test_kd, y_prob_det))\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.legend()\nplt.savefig('precision_recall_curve.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úÖ Calibration curve saved as 'calibration_curve.png'\")\nprint(\"‚úÖ Precision-Recall curve saved as 'precision_recall_curve.png'\")\n\n# =============================================================================\n# STEP 8B: DECISION CURVE ANALYSIS (CLINICAL PROGNOSIS MODEL)\n# =============================================================================\n\nif SURVIVAL_AVAILABLE:\n    print(\"\\nüìà Decision Curve Analysis (Clinical Prognosis Model)...\")\n    \n    # Simple Decision Curve Approximation\n    times = [12, 24, 36, 60]  # months\n    surv_funcs = prognosis_models[\"Random Survival Forest\"].predict_survival_function(X_test_cl)\n    \n    plt.figure(figsize=(8,6))\n    for i, t in enumerate(times):\n        risks = [1 - fn(t) for fn in surv_funcs]  # risk = 1-survival\n        plt.hist(risks, bins=20, alpha=0.5, label=f\"Risk at {t} months\")\n    \n    plt.xlabel(\"Predicted Risk Probability\")\n    plt.ylabel(\"Patient Count\")\n    plt.title(\"Decision Distribution (Clinical Prognosis Model)\")\n    plt.legend()\n    plt.savefig('decision_curve_analysis.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"‚úÖ Decision curve analysis saved as 'decision_curve_analysis.png'\")\n\n# =============================================================================\n# STEP 8C: EXPLAINABILITY ‚Äì SHAP\n# =============================================================================\n\nprint(\"\\nüîç Explainability Analysis - SHAP...\")\n\ntry:\n    # SHAP for Detection Model\n    explainer_det = shap.Explainer(calibrated_clf, X_train_proc)\n    shap_values_det = explainer_det(X_test_proc[:50])\n    \n    # Global summary\n    plt.figure(figsize=(10,8))\n    shap.summary_plot(shap_values_det, X_test_proc[:50], plot_type=\"bar\", show=False)\n    plt.title(\"SHAP Feature Importance (Detection Model)\")\n    plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    # Dependence plot (Age vs Creatinine) - using feature indices\n    try:\n        # Find indices for age and creatinine features in preprocessed data\n        feature_names = [f\"feature_{i}\" for i in range(X_test_proc.shape[1])]\n        age_idx = None\n        sc_idx = None\n        \n        # Try to find age and creatinine features by looking at original column names\n        if \"age\" in X_kd.columns:\n            age_idx = list(X_kd.columns).index(\"age\")\n        if \"sc\" in X_kd.columns:\n            sc_idx = list(X_kd.columns).index(\"sc\")\n        \n        if age_idx is not None and sc_idx is not None:\n            plt.figure(figsize=(8,6))\n            shap.dependence_plot(sc_idx, shap_values_det.values, X_test_proc[:50], \n                               interaction_index=age_idx, show=False)\n            plt.title(\"SHAP Dependence Plot: Creatinine vs Age\")\n            plt.savefig('shap_dependence.png', dpi=300, bbox_inches='tight')\n            plt.show()\n    except Exception as e:\n        print(f\"‚ö†Ô∏è  SHAP dependence plot error: {e}\")\n    \n    print(\"‚úÖ SHAP summary plot saved as 'shap_summary.png'\")\n    print(\"‚úÖ SHAP dependence plot saved as 'shap_dependence.png'\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  SHAP analysis error: {e}\")\n\n# =============================================================================\n# STEP 8D: EXPLAINABILITY ‚Äì LIME\n# =============================================================================\n\nprint(\"\\nüîç Explainability Analysis - LIME...\")\n\ntry:\n    lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n        training_data=np.array(X_train_proc),\n        feature_names=[f\"feature_{i}\" for i in range(X_train_proc.shape[1])],\n        class_names=[\"Not CKD\",\"CKD\"],\n        mode=\"classification\"\n    )\n    \n    # Explain first test sample\n    exp = lime_explainer.explain_instance(\n        data_row=X_test_proc[0],\n        predict_fn=calibrated_clf.predict_proba\n    )\n    \n    print(\"‚úÖ LIME explainer created\")\n    print(\"‚úÖ LIME explanation generated for first test sample\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  LIME analysis error: {e}\")\n\n# =============================================================================\n# STEP 8E: GLOBAL SURROGATE MODEL\n# =============================================================================\n\nprint(\"\\nüå≥ Global Surrogate Model...\")\n\ntry:\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn import tree\n    \n    # Surrogate decision tree trained on ensemble predictions\n    y_surrogate = calibrated_clf.predict(X_train_proc)\n    \n    surrogate = DecisionTreeClassifier(max_depth=3, random_state=42)\n    surrogate.fit(X_train_proc, y_surrogate)\n    \n    plt.figure(figsize=(12,8))\n    tree.plot_tree(surrogate, feature_names=[f\"feature_{i}\" for i in range(X_train_proc.shape[1])], \n                   class_names=[\"Not CKD\",\"CKD\"], filled=True)\n    plt.title(\"Global Surrogate Model for Detection Ensemble\")\n    plt.savefig('surrogate_tree.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"‚úÖ Surrogate decision tree saved as 'surrogate_tree.png'\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Surrogate model error: {e}\")\n\n# =============================================================================\n# SAVE MODELS\n# =============================================================================\n\nprint(\"\\nüíæ Saving Trained Models...\")\n\ntry:\n    # Save detection model\n    joblib.dump(calibrated_clf, 'best_detection_model.pkl')\n    joblib.dump(preprocessor_kd, 'detection_preprocessor.pkl')\n    \n    # Save prognosis models if available\n    if SURVIVAL_AVAILABLE:\n        joblib.dump(prognosis_models, 'prognosis_models.pkl')\n        joblib.dump(preprocessor_cl, 'prognosis_preprocessor.pkl')\n    \n    # Save results\n    results_detection_df.to_csv('detection_results.csv', index=False)\n    \n    print(\"‚úÖ Best detection model saved as 'best_detection_model.pkl'\")\n    print(\"‚úÖ Detection preprocessor saved as 'detection_preprocessor.pkl'\")\n    if SURVIVAL_AVAILABLE:\n        print(\"‚úÖ Prognosis models saved as 'prognosis_models.pkl'\")\n        print(\"‚úÖ Prognosis preprocessor saved as 'prognosis_preprocessor.pkl'\")\n    print(\"‚úÖ Results saved as 'detection_results.csv'\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Model saving error: {e}\")\n\n# =============================================================================\n# PIPELINE SUMMARY\n# =============================================================================\n\nprint(\"\\nüéâ CKD PREDICTION PIPELINE COMPLETED!\")\nprint(\"=\" * 60)\n\nprint(\"\\nüìä PIPELINE SUMMARY:\")\nprint(f\"‚úÖ Detection Models: {len(base_classifiers)} trained and evaluated\")\nprint(f\"‚úÖ Best Detection Model: {best_model_name}\")\nprint(f\"‚úÖ Cross-Validation AUC: {cv_scores[best_model_name]:.3f} ¬± {cv_results[best_model_name]['AUC_std']:.3f}\")\nprint(f\"‚úÖ Model Calibration: Completed\")\n\nif SURVIVAL_AVAILABLE:\n    print(f\"‚úÖ Prognosis Models: CoxPH + Random Survival Forest\")\n    print(f\"‚úÖ Prognosis Evaluation: Training & Test metrics (Acc, F1, Recall, Precision, AUC, MAE)\")\n    print(f\"‚úÖ Hybrid Decision Function: Available\")\nelse:\n    print(f\"‚ö†Ô∏è  Prognosis Models: Not available (scikit-survival required)\")\n\nprint(f\"‚úÖ Evaluation Metrics: Calibration curves, PR curves\")\nprint(f\"‚úÖ Explainability: SHAP + LIME + Surrogate trees\")\nprint(f\"‚úÖ Visualizations: Saved as PNG files\")\nprint(f\"‚úÖ Models: Saved as PKL files\")\n\nprint(\"\\nüöÄ PIPELINE READY FOR CLINICAL USE!\")\nprint(\"=\" * 60)\n\n# =============================================================================\n# USAGE EXAMPLE\n# =============================================================================\n\nprint(\"\\nüìñ USAGE EXAMPLE:\")\nprint(\"\"\"\n# Load saved models\ndetection_model = joblib.load('best_detection_model.pkl')\npreprocessor = joblib.load('detection_preprocessor.pkl')\n\n# Prepare new patient data (same format as training)\nnew_patient = pd.DataFrame({\n    'age': [65],\n    'bp': [120],\n    'sg': [1.02],\n    # ... other features\n})\n\n# Preprocess and predict\nnew_patient_proc = preprocessor.transform(new_patient)\nprediction = detection_model.predict(new_patient_proc)\nprobability = detection_model.predict_proba(new_patient_proc)[:,1]\n\nprint(f\"CKD Prediction: {'Present' if prediction[0] == 1 else 'Not Present'}\")\nprint(f\"Confidence: {probability[0]:.3f}\")\n\"\"\")\n\nprint(\"\\nüè• Thank you for using the CKD Prediction Pipeline!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}